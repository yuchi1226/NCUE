1.
{
    (1)D
    (2)D
    (3)A
    (4)D
    (5)B
    (6)C
    (7)B(https://quizlet.com/463560214/operating-systems-ch11-flash-cards/)
    (8)A
    (9)C(https://quizlet.com/tw/557655835/quiz-7-synchronization-examples-flash-cards/)
    (10)B
}
2.
{
    (1)160、40
    (2)25
    (3)55
}
3.
{
    (a) 儲存系統設計導致尾部延遲的四個可能原因：
    軟體方面:
    1. shared resources: 在同台機器上面 CPU, memory, networking 的競爭。
    2. daemon & maintaince activities: 一些 backgroud processes 不定時的被啟動執行，造成原本的 request response time 受到影響。(e.g data compaction & garbage collection)
    硬體方面:
    3. Power limits: CPU 被設計成可以暫時跑超過他的平均功率上，而得到效能提升，但是如果這情況發生太久就會被 throttle 降溫。
    4. SSD GC: SSD 發生 GC 的時候，也會變慢

    (b) 解決長尾延遲問題的兩種方法：
    Differentiating service classes and higher-level queuing: 將請求分類，讓一些需要 interactive 的請求能夠先被處理。
    Reducing head-of-line blocking: 長時間的請求常常會造成 tail latency，我們可以把這種請求，打散成一連串小的請求，這樣可以跟其他的小請求交互執行，讓這些小請求不會被卡在 queue 後面，造成整體的 tail latency 變差。
    Managing background activities and synchronized disruption: 將 background process 拆解成成本較低的操作，並且搭配 throttle 的功能，然後在整體 system loading 比較低的時候再去執行。
    Hedged requests: 將同樣的 request 發給不同的 servers，然後拿先返回的 response 回傳給 user，一但拿到 response，就把另外未處理完的 request 取消掉，但這件事大家一定有疑問，不是會造成 server loading 加重嗎？其中論文裡面的一個方法是，只有當第一個 request 到 95th 的時間時還沒拿到 response，才再發出第二個 request，這樣的做法約略讓整體的 loading 提升 5%，但是卻可以有效地減低 tail latency。
    Tied requests: 跟 Hedged requests 的做法差不多，只是這裡是直接把 requests 發給多個 server 的 queue，而不是等待一段時間再發，server 做完後會主動取消其他的 server 的 request，不過這件事會增加網路的負擔
    Micro-partition: 將一台大 server 切割成多個小 partition，類似一台機器切成 20 個 partition，這樣可以在動態任務分配(dynamic assignment)上面更靈活，主要就是讓 granularity 切得更細。
    Selectively increase replication factors: 當一個 partition 變成 hot 的時候，能夠動態增加 replica 去 share loading。(想法: 這個目前有很多 autoscaling 的技術，著重的就是這個點)
    Put slow machines on probation: 當機器變慢的時候，將他隔離起來，這也是 circuit breaker，熔斷讓 machine 的情況不會變更差。
    Consider ‘good enough’ responses: 服務降級，讓 service 還是有回應，但是給的內容也許不完整。
}
4.bard
{
    (1) 一個進程最多可以有 2^32/ 4K = 2^20 個頁
        物理地址的最大位元數是 35 位元
        系統的最大框架數是 35 GB / 4K = 2^23

    (2) EAT(effective access time)= P x hit memory time + (1-P) x miss memory time
        hit memory time=  TLB access time + memory access time 
        miss memory time= TLB access time + 2*memory access time 
        P=0.8

    (3) 

    (4)6
}
5.
{
    (1) 下圖顯示了單周期數據路徑，其中標識了流水線階段。為了構建流水線版本的數據路徑，應添加哪些硬體資源來緩衝所需數據在/跨流水線階段之間？為什麼？
    要構建流水線數據路徑，需要添加以下硬體資源：
    **流水線寄存器：**這些寄存器位於每個流水線階段之間。它們存儲下一個階段所需的數據，以便各個階段可以獨立且並行地運行。
    以下是為什麼需要流水線寄存器的原因：
    在單周期數據路徑中，每條指令需要一個時鐘週期才能完成。在流水線數據路徑中，每條指令都被分解為較小的階段，這些階段可以並行執行。這意味著可以同時在管道中存在多條指令。
    但是，各階段之間存在依賴關係。例如，EX階段需要ID階段的輸出才能繼續。如果沒有流水線寄存器，EX階段將不得不等待ID階段完成處理前一個指令才能開始處理當前指令。這將違背流水線的目的，因為它不會提高數據路徑的吞吐量。
    因此，流水線寄存器對於流水線數據路徑的有效運行至關重要。它們允許每個階段同時處理不同的指令，同時仍確保各階段之間的數據依賴關係得到滿足。

    (2) 
    在數據路徑中添加轉發單元。轉發單元檢查當前指令的源操作數是否與 MEM 或 WB 階段中的前一個指令的目的寄存器相同。如果是，則轉發單元將繞過寄存器文件，直接將數據從 MEM 或 WB 階段發送到 EX 階段。
    這確保 EX 階段始終可以訪問正確的源操作數，無論其是否正在從內存中加載。

    (3) 表中列出了單周期數據通路五個階段的操作時間。
    處理加載指令的延遲為：530 ps
    處理存儲指令的延遲為：450 ps

    (4) 繼續上述問題。單周期數據通路的流水線執行時鐘週期為：
    流水線執行時鐘週期是指流水線可以執行一個指令所需的時間。
    在單周期數據通路中，每個階段都需要等待其前一個階段完成才能開始執行。因此，流水線執行時鐘週期等於單個階段的操作時間。
    流水線執行時鐘週期 = 最長階段的操作時間
    流水線執行時鐘週期 = 200 ps
    因此，單周期數據通路的流水線執行時鐘週期為 200 ps。
}
6.
{
    (1) addi x10, x14, 10
        NOP 
        NOP
        sub x13, x10, x14
        xor x4, x3, x5

    (2) addi x10, x14, 10
        xor x4, x3, x5
        NOP
        sub x13, x10, x14

    (3)Always-Taken 預測器: 此預測器始終預測分支將被 taken。在這種情況下，它正確預測了 4 個分支中的 5 個（4 個 taken，1 個 not-taken）。因此，精確度為：
    Always-Taken 精確度 = 4 個正確預測 / 5 個總分支 = 0.8
    Always-Not-Taken 預測器: 此預測器始終預測分支將不會被 taken。在這種情況下，它錯誤預測了 4 個分支中的 5 個（4 個 taken，1 個 not-taken）。因此，精確度為：
    Always-Not-Taken 精確度 = 1 個正確預測 / 5 個總分支 = 0.2
    比較精確度，我們可以看到在這種情況下 Always-Taken 預測器表現更好，精確度為 0.8，而 Always-Not-Taken 預測器為 0.2。這是因為迴圈迭代多次，使分支更有可能被 taken。
    因此，對於這個特定的迴圈程式碼段，Always-Taken 預測器將更有效地減少控制異常。
}